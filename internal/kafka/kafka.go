package kafka

import (
	"context"
	"errors"
	"fmt"
	"strconv"
	"strings"
	"time"

	env "github.com/csye7125-su24-team06/webapp-cve-consumer/internal"
	"github.com/csye7125-su24-team06/webapp-cve-consumer/internal/function"
	"github.com/segmentio/kafka-go"
	"github.com/segmentio/kafka-go/sasl/scram"
	"github.com/sirupsen/logrus"
)

type KafkaConsumerInterface interface {
	Consume()
	Close() error
	Ping() error
	IsClosed() bool
}

type kafkaConsumer struct {
	count    int
	client   *kafka.Reader
	conns    map[string]*kafka.Conn
	topic    string
	dialer   *kafka.Dialer
	brokers  []string
	isClosed bool
}

var KafkaConsumerObject KafkaConsumerInterface

func GetKafkaConsumer() KafkaConsumerInterface {

	topic := env.GetEnvOrDefault("KAFKA_TOPIC", "cve")

	kafkaService := env.GetEnvOrDefault("KAFKA_SERVICE", "")
	brokersEnv := env.GetEnvOrDefault("KAFKA_BROKERS", "localhost:9092,localhost:9093,localhost:9094")
	brokers := strings.Split(brokersEnv, ",")
	group_id := env.GetEnvOrDefault("KAFKA_CONSUMER_GROUP_ID", "cve")
	maxBytes, err := strconv.ParseFloat(env.GetEnvOrDefault("KAFKA_MAX_SIZE", "10e6"), 64)
	if err != nil {
		panic(fmt.Sprintf("Invalid Max Bytes : %v", err))
	}

	dialer := &kafka.Dialer{
		Timeout:   10 * time.Second,
		DualStack: true,
	}

	username := env.GetEnvOrDefault("KAFKA_USERNAME", "")
	password := env.GetEnvOrDefault("KAFKA_PASSWORD", "")
	if username != "" || password != "" {
		auth, err := scram.Mechanism(scram.SHA512, username, password)
		if err != nil {
			logrus.Fatalf("Failed to create auth: %v", err)
		}
		dialer.SASLMechanism = auth
	}

	if kafkaService != "" {
		conn, err := dialer.DialContext(context.Background(), "tcp", kafkaService)
		if err != nil {
			logrus.Fatalf("Failed to connect to Kafka Service: %v", err)
			return nil
		}
		defer conn.Close()
		brokersFromService, err := conn.Brokers()
		if err != nil {
			logrus.Fatalf("Failed to fetch brokers: %v", err)
			return nil
		}
		brokers = []string{}
		for _, broker := range brokersFromService {
			brokers = append(brokers, fmt.Sprintf("%s:%d", broker.Host, broker.Port))
		}
	}

	r := kafka.NewReader(kafka.ReaderConfig{
		Brokers:  brokers,
		GroupID:  group_id,
		Topic:    topic,
		MaxBytes: int(maxBytes),
		Dialer:   dialer,
	})

	KafkaConsumerObject = &kafkaConsumer{
		client:   r,
		conns:    make(map[string]*kafka.Conn),
		topic:    topic,
		brokers:  brokers,
		dialer:   dialer,
		isClosed: false,
		count:    0,
	}

	return KafkaConsumerObject
}

func (kc *kafkaConsumer) Consume() {

	logrus.Println("Starting Kafka Consumer...")
	for {
		m, err := kc.client.FetchMessage(context.Background())
		if err != nil {
			if !kc.isClosed {
				logrus.Printf("Kafka Client Faliure : %v", err)
			}
			break
		}

		// Process the message
		err = function.ProcessCve(&m.Value)

		// If database processor succeeds, commit the message
		if err == nil {
			if err := kc.client.CommitMessages(context.Background(), m); err != nil {
				logrus.Printf("Kafka Client Failed to commit message : %v", err)
				break
			}
			kc.count++
			env.ConsumedMessages.Inc()
		}

		if kc.count%1000 == 0 {
			logrus.Printf("Records Processed: %v", kc.count)
		}
	}

	logrus.Printf("Total messages processed in the routine : %v", kc.count)
}

func (kc *kafkaConsumer) Close() error {
	kc.isClosed = true

	if err := kc.client.Close(); err != nil {
		return err
	}

	for _, conn := range kc.conns {
		conn.Close()
	}

	return nil
}

func (kc *kafkaConsumer) Ping() error {

	for _, broker := range kc.brokers {
		if kc.conns[broker] == nil {
			conn, err := kc.dialer.DialContext(context.Background(), "tcp", broker)
			if err == nil {
				kc.conns[broker] = conn
			}
		}
	}

	var fetchedPartitions []kafka.Partition

	for _, conn := range kc.conns {
		partitions, err := conn.ReadPartitions()
		if err == nil {
			fetchedPartitions = partitions
			break
		}
	}

	for _, partition := range fetchedPartitions {
		if partition.Topic == kc.topic {
			return nil
		}
	}

	for _, broker := range kc.brokers {
		kc.conns[broker] = nil
	}

	return errors.New("unable to find topic")
}

func (kc *kafkaConsumer) IsClosed() bool {
	return kc.isClosed
}
